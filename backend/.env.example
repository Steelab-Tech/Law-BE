# LLM Configuration
LLM_MODEL_ID=1TuanPham/T-VisStar-7B-v0.1
LLM_DEVICE=cuda:0

# Remote LLM API (Google Colab with Cloudflare Tunnel)
# Set this to use remote inference instead of local GPU
# Leave empty or remove to use local GPU inference
# LLM_API_URL=https://xxxx.trycloudflare.com

# API Keys
TAVILY_API_KEY=tvly-...

# Celery Configuration
CELERY_BROKER_URL=redis://localhost:6379
CELERY_RESULT_BACKEND=redis://localhost:6379

# Database Configuration
MONGODB_URL=mongodb://localhost:27017
QDRANT_URL=localhost:6333
ELASTICSEARCH_URL=localhost:9200
REDIS_URL=localhost:6379
