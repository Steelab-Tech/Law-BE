# LLM Configuration
LLM_MODEL_ID=1TuanPham/T-VisStar-7B-v0.1
LLM_DEVICE=cuda:0

# Remote LLM API (Google Colab with Cloudflare Tunnel)
# Set this to use remote inference instead of local GPU
# Leave empty or remove to use local GPU inference
# LLM_API_URL=https://xxxx.trycloudflare.com

# API Keys
TAVILY_API_KEY=tvly-...

# Celery Configuration (use Docker service names)
CELERY_BROKER_URL=redis://redis:6379
CELERY_RESULT_BACKEND=redis://redis:6379

# Database Configuration (use Docker service names)
MONGODB_URL=mongodb://mongodb:27017
ELASTICSEARCH_URL=elasticsearch:9200
REDIS_URL=redis:6379

# Qdrant Configuration (local Docker)
QDRANT_URL=qdrant:6333
