{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-VisStar-7B LLM Server\n",
    "\n",
    "This notebook runs T-VisStar-7B model as an API server on Google Colab with Cloudflare Tunnel.\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with GPU runtime (T4 recommended)\n",
    "- Runtime > Change runtime type > GPU\n",
    "\n",
    "**Usage:**\n",
    "1. Run all cells\n",
    "2. Copy the Cloudflare URL from the last cell output\n",
    "3. Set `LLM_API_URL` in your backend `.env` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install dependencies\n",
    "!pip install -q transformers accelerate bitsandbytes flask\n",
    "\n",
    "# Download Cloudflare Tunnel binary\n",
    "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\n",
    "!chmod +x cloudflared\n",
    "\n",
    "print(\"Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load model with 4-bit quantization\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Model configuration\n",
    "MODEL_ID = os.getenv(\"LLM_MODEL_ID\", \"1TuanPham/T-VisStar-7B-v0.1\")\n",
    "\n",
    "print(f\"Loading model: {MODEL_ID}\")\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# 4-bit quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.padding_side = 'left'\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Create Flask API server\n",
    "from flask import Flask, request, jsonify\n",
    "import threading\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/health\", methods=[\"GET\"])\n",
    "def health():\n",
    "    \"\"\"Health check endpoint.\"\"\"\n",
    "    return jsonify({\n",
    "        \"status\": \"ok\",\n",
    "        \"model\": MODEL_ID,\n",
    "        \"gpu\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n",
    "    })\n",
    "\n",
    "@app.route(\"/chat/complete\", methods=[\"POST\"])\n",
    "def chat_complete():\n",
    "    \"\"\"Chat completion endpoint - compatible with LocalLLM interface.\"\"\"\n",
    "    try:\n",
    "        data = request.json\n",
    "        messages = data.get(\"messages\", [])\n",
    "        max_new_tokens = data.get(\"max_new_tokens\", 512)\n",
    "        temperature = data.get(\"temperature\", 0.7)\n",
    "\n",
    "        logger.info(f\"Generating response: max_tokens={max_new_tokens}, temp={temperature}\")\n",
    "\n",
    "        # Apply chat template\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=temperature > 0,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        # Decode only new tokens\n",
    "        response = tokenizer.decode(\n",
    "            outputs[0][inputs['input_ids'].shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Generated {len(response)} characters\")\n",
    "        return jsonify({\"response\": response.strip()})\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {e}\")\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "# Run Flask in background thread\n",
    "def run_server():\n",
    "    app.run(host='0.0.0.0', port=5000, debug=False, use_reloader=False)\n",
    "\n",
    "server_thread = threading.Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "print(\"Flask server started on port 5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Start Cloudflare Tunnel\n",
    "import subprocess\n",
    "import re\n",
    "import time\n",
    "\n",
    "print(\"Starting Cloudflare Tunnel...\")\n",
    "\n",
    "# Start cloudflared tunnel\n",
    "process = subprocess.Popen(\n",
    "    ['./cloudflared', 'tunnel', '--url', 'http://localhost:5000'],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "# Wait for tunnel to establish\n",
    "time.sleep(3)\n",
    "\n",
    "# Read stderr to find the URL\n",
    "public_url = None\n",
    "for _ in range(20):  # Try for up to 20 lines\n",
    "    line = process.stderr.readline()\n",
    "    if not line:\n",
    "        time.sleep(0.5)\n",
    "        continue\n",
    "    print(line.strip())  # Debug output\n",
    "    match = re.search(r'https://[a-z0-9-]+\\.trycloudflare\\.com', line)\n",
    "    if match:\n",
    "        public_url = match.group()\n",
    "        break\n",
    "\n",
    "if public_url:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"LLM SERVER READY!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nPublic URL: {public_url}\")\n",
    "    print(f\"\\nAdd to your backend .env file:\")\n",
    "    print(f\"LLM_API_URL={public_url}\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "else:\n",
    "    print(\"\\nFailed to get Cloudflare URL. Check output above for errors.\")\n",
    "    print(\"You may need to restart this cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Test the API (optional)\n",
    "import requests\n",
    "\n",
    "# Test health endpoint\n",
    "response = requests.get(\"http://localhost:5000/health\")\n",
    "print(\"Health check:\", response.json())\n",
    "\n",
    "# Test chat completion\n",
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n",
    "]\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:5000/chat/complete\",\n",
    "    json={\"messages\": test_messages, \"max_new_tokens\": 50, \"temperature\": 0.7}\n",
    ")\n",
    "print(\"\\nChat response:\", response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Keep notebook alive\n",
    "# Run this cell to prevent Colab from timing out\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "print(\"Keeping notebook alive... (Ctrl+C or stop cell to exit)\")\n",
    "print(f\"LLM API URL: {public_url}\" if public_url else \"URL not available\")\n",
    "\n",
    "counter = 0\n",
    "while True:\n",
    "    counter += 1\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Server running... Uptime: {counter} minutes\")\n",
    "    print(f\"LLM API URL: {public_url}\" if public_url else \"URL not available\")\n",
    "    time.sleep(60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
