{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Vietnamese Legal Embeddings\n",
    "\n",
    "This notebook generates embeddings using Colab's free T4 GPU and exports them to a file.\n",
    "Then you can import the file into your local Qdrant without needing GPU.\n",
    "\n",
    "## Steps:\n",
    "1. Run all cells to generate embeddings (~15-20 min)\n",
    "2. Download the output file `embeddings.jsonl`\n",
    "3. Run `python import_embeddings_local.py` on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install dependencies\n",
    "!pip install -q datasets sentence-transformers tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration\n",
    "MODEL_NAME = \"minhquan6203/paraphrase-vietnamese-law\"\n",
    "BATCH_SIZE = 64  # Larger batch for GPU\n",
    "OUTPUT_FILE = \"embeddings.jsonl\"\n",
    "\n",
    "# Chunking config\n",
    "CHUNK_SIZE = 512  # Characters per chunk\n",
    "CHUNK_OVERLAP = 100  # Overlap between chunks\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Output: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Check GPU & Import libraries\n",
    "import torch\n",
    "import re\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"WARNING: No GPU detected! This will be slow.\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load dataset\n",
    "print(\"Loading dataset: vietnamese-legal-corpus-20k-raw...\")\n",
    "print(\"(This may take a few minutes for 2.6GB download)\")\n",
    "dataset = load_dataset(\"52100303-TranPhuocSang/vietnamese-legal-corpus-20k-raw\")\n",
    "print(f\"Dataset loaded: {len(dataset['train'])} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Load embedding model\n",
    "print(f\"Loading model: {MODEL_NAME}...\")\n",
    "model = SentenceTransformer(MODEL_NAME, device=device)\n",
    "print(f\"Model loaded! Embedding dimension: {model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Text processing functions\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and normalize text\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "    return text.strip()\n",
    "\n",
    "def chunk_text(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):\n",
    "    \"\"\"Split text into overlapping chunks\"\"\"\n",
    "    if not text or len(text) < 100:\n",
    "        return [text] if text else []\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "\n",
    "        # Try to cut at sentence boundary\n",
    "        if end < len(text):\n",
    "            last_period = chunk.rfind('.')\n",
    "            last_newline = chunk.rfind('\\n')\n",
    "            cut_point = max(last_period, last_newline)\n",
    "            if cut_point > chunk_size // 2:\n",
    "                chunk = chunk[:cut_point + 1]\n",
    "                end = start + cut_point + 1\n",
    "\n",
    "        if chunk.strip():\n",
    "            chunks.append(chunk.strip())\n",
    "\n",
    "        start = end - overlap\n",
    "        if start >= len(text):\n",
    "            break\n",
    "\n",
    "    return chunks\n",
    "\n",
    "print(\"Text processing functions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Process and chunk all documents\n",
    "print(\"Processing and chunking all documents...\")\n",
    "\n",
    "all_chunks = []\n",
    "for idx, doc in enumerate(tqdm(dataset['train'], desc=\"Chunking\")):\n",
    "    full_text = clean_text(doc.get('full_text', ''))\n",
    "    title = clean_text(doc.get('title', ''))\n",
    "\n",
    "    metadata = {\n",
    "        'title': title,\n",
    "        'official_number': doc.get('official_number', ''),\n",
    "        'document_type': doc.get('document_type', ''),\n",
    "        'document_field': doc.get('document_field', ''),\n",
    "        'issued_date': doc.get('issued_date', ''),\n",
    "        'effective_date': doc.get('effective_date', ''),\n",
    "        'place_issue': doc.get('place_issue', ''),\n",
    "        'signer': doc.get('signer', ''),\n",
    "        'url': doc.get('url', ''),\n",
    "        'source_id': doc.get('source_id', idx),\n",
    "    }\n",
    "\n",
    "    if full_text:\n",
    "        chunks = chunk_text(full_text)\n",
    "        for chunk_idx, chunk in enumerate(chunks):\n",
    "            all_chunks.append({\n",
    "                'text': chunk,\n",
    "                'chunk_idx': chunk_idx,\n",
    "                'total_chunks': len(chunks),\n",
    "                **metadata\n",
    "            })\n",
    "    elif title:\n",
    "        all_chunks.append({\n",
    "            'text': title,\n",
    "            'chunk_idx': 0,\n",
    "            'total_chunks': 1,\n",
    "            **metadata\n",
    "        })\n",
    "\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Generate embeddings and save to file\n",
    "print(f\"Generating embeddings and saving to {OUTPUT_FILE}...\")\n",
    "print(f\"This will process {len(all_chunks)} chunks in batches of {BATCH_SIZE}\")\n",
    "\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "    for i in tqdm(range(0, len(all_chunks), BATCH_SIZE), desc=\"Embedding\"):\n",
    "        batch = all_chunks[i:i+BATCH_SIZE]\n",
    "        \n",
    "        # Create embeddings\n",
    "        texts = [chunk['text'] for chunk in batch]\n",
    "        embeddings = model.encode(\n",
    "            texts,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            normalize_embeddings=True,\n",
    "            show_progress_bar=False\n",
    "        )\n",
    "        \n",
    "        # Write each point as JSON line\n",
    "        for j, (chunk, embedding) in enumerate(zip(batch, embeddings)):\n",
    "            point = {\n",
    "                'id': i + j,\n",
    "                'vector': embedding.tolist(),\n",
    "                'payload': chunk\n",
    "            }\n",
    "            f.write(json.dumps(point, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        # Clear GPU memory periodically\n",
    "        if torch.cuda.is_available() and (i // BATCH_SIZE) % 50 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\nDone! Embeddings saved to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Check output file\n",
    "import os\n",
    "\n",
    "file_size = os.path.getsize(OUTPUT_FILE) / (1024 * 1024 * 1024)  # GB\n",
    "line_count = sum(1 for _ in open(OUTPUT_FILE, 'r', encoding='utf-8'))\n",
    "\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"EXPORT COMPLETE!\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Output file: {OUTPUT_FILE}\")\n",
    "print(f\"File size: {file_size:.2f} GB\")\n",
    "print(f\"Total vectors: {line_count}\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"1. Download {OUTPUT_FILE} from Colab\")\n",
    "print(f\"2. Place it in your project's colab/ folder\")\n",
    "print(f\"3. Run: python colab/import_embeddings_local.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Download file (run this to trigger download in Colab)\n",
    "from google.colab import files\n",
    "files.download(OUTPUT_FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
